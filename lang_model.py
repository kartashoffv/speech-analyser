from langchain_community.llms import Ollama

llm = Ollama(model="openchat")

from langchain_core.prompts import ChatPromptTemplate
prompt = ChatPromptTemplate.from_template("""Ответьте на вопрос на основе следующей инструкции:
Вы — внимательный слушатель диалога, в котором общаются дежурный и машинист. 
DSP дает инструкции машинисту, машинист должен воспроизвести инструкции дежурный. 
После этого дежурный подтверждает, правильны ли были повторены инструкции или нет.

Ваша задача заключается в следующем:
Выявить, правильно ли машинист воспроизвел команду дежурного (очень важны детали: номера путей, время) 
Правильно ли дежурный подтвердил слова машиниста (если машинист неправильно произвел команду, а дежурный ее подтвердил, это ошибка)

Формат диалога на вход следующий:
Дежурный: [Обращение]
Машинист: [Ответ]
Дежурный: [Инструкция]
Машинист: [Воспроизведение команды диспетчера]
Дежурный: [Подтверждение или исправление машиниста]

* В диалоге могут присутствовать аналогичные модули

Инструкции:
Если машинист правильно воспроизвел инструкции, выведите "ВСЕ ВЕРНО"
Если машинист ошибается в повторении инструкций (например, неправильный путь, время и т.д.), выведите "ДОПУЩЕНА ОШИБКА"
Ваши ответы ВСЕГДА должны соответстовать инструкции выше и выводить ТОЛЬКО один из следующих вариантов: "ВСЕ ВЕРНО", "ДОПУЩЕНА ОШИБКА".

INPUT: {input}
На основе вышеизложенного анализа, выберите одно из двух значений: "ВСЕ ВЕРНО" или "ДОПУЩЕНА ОШИБКА".""")

chain = prompt | llm 

def txt_classifier(voice2text_config):
    msgs = [msg['text'] for msg in voice2text_config]
    msg = " ".join(msgs)
    ans = chain.invoke({"input": msg})
    if not "ДОПУЩЕНА ОШИБКА" in ans or not "ВСЕ ВЕРНО" in ans:
        return "ДИАЛОГ НЕ ОБРАБОТАН"
    else:
        return ans

# Пример работы    
#chunks = [{'timestamp': (0.0, 15.0), 'text': ''}, {'timestamp': (20.0, 52.24), 'text': ''}]
#print(txt_classifier(chunks))
